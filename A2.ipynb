{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1x_8vd4fdI0iH1h43_KCmrZepP3PQKgwi","authorship_tag":"ABX9TyNxgU4GqBya8KzqbXvOvGF8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Libraries**"],"metadata":{"id":"f9gsc9upnMaq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJLCFHhgm9cQ"},"outputs":[],"source":["import nltk\n","import numpy as np\n","import string\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchtext, math\n","from tqdm import tqdm\n","from nltk import sent_tokenize\n","import pickle"]},{"cell_type":"markdown","source":["Downloading the punkt for the tokenization support"],"metadata":{"id":"V-edIw9KU1ZM"}},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuGtR83uyDNK","executionInfo":{"status":"ok","timestamp":1706369445852,"user_tz":-420,"elapsed":674,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"0799e80f-07bb-40d6-94ad-02d6e2fc852b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Defing the device"],"metadata":{"id":"uboIMjiuVSTs"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBwrtqhfSZ5Y","executionInfo":{"status":"ok","timestamp":1706369445852,"user_tz":-420,"elapsed":4,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"8bd9ccdd-b2c9-47cb-c2ab-115d05a403de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"Lw6LhvJeSbw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Loading the dataset**"],"metadata":{"id":"AvKgmqsynR7S"}},{"cell_type":"code","source":["path = '/content/drive/MyDrive/NLP_A2/game_of_thrones.txt'\n","\n","# Open the file in read mode ('r')\n","with open(path, 'r') as file:\n","    # Read the entire content of the file\n","    file_content = file.read()\n","\n","# Print or manipulate the content as needed\n","print(file_content[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjlYZTIfnWAN","executionInfo":{"status":"ok","timestamp":1706369446213,"user_tz":-420,"elapsed":363,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"a1029736-9a09-4296-94ca-e837a6d2a045"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A Song of Ice and Fire\n","\n","A Game of Thrones\n","\n","PROLOGUE\n","\n","We should start back, Gared urged as the woods \n"]}]},{"cell_type":"markdown","source":["**Preprocessing**"],"metadata":{"id":"QqfYLHDi0rLY"}},{"cell_type":"markdown","source":["1. Remove the tabs and Newlines"],"metadata":{"id":"Evup3Co3MfwI"}},{"cell_type":"code","source":["def remove_tabs_newlines(text):\n","  text = text.replace('\\n','').replace('\\t','')\n","  return text"],"metadata":{"id":"5axmgpTSMlvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["removed_TabsNewlines = remove_tabs_newlines(file_content)\n","removed_TabsNewlines[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"075ilpKQMlyH","executionInfo":{"status":"ok","timestamp":1706369446213,"user_tz":-420,"elapsed":6,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"820863a3-b216-49fd-e901-74c3d6abc80a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A Song of Ice and FireA Game of ThronesPROLOGUEWe should start back, Gared urged as the woods began '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["2. Tokenizing into sentences"],"metadata":{"id":"z6hRqNjAFSiZ"}},{"cell_type":"code","source":["def tokenize_sentence(text):\n","  sentences = sent_tokenize(text)\n","  return sentences"],"metadata":{"id":"HlPQZnfqFPni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences = tokenize_sentence(removed_TabsNewlines)\n","sentences[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzWTc0G1FRm5","executionInfo":{"status":"ok","timestamp":1706369446580,"user_tz":-420,"elapsed":372,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"228c9272-1ae6-4087-c197-2e77e69107c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A Song of Ice and FireA Game of ThronesPROLOGUEWe should start back, Gared urged as the woods began to grow dark around them.',\n"," 'The wildlings are dead.Do the dead frighten you?',\n"," 'Ser Waymar Royce asked with just the hint of a smile.Gared did not rise to the bait.',\n"," 'He was an old man, past fifty, and he had seen the lordlings come and go.',\n"," 'Dead is dead, he said.']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# def remove_punctuation(text):\n","#     # Create a translation table to map each punctuation character to None\n","#     translator = str.maketrans('', '', string.punctuation)\n","\n","#     # Use translate to remove punctuations from the text\n","#     text_without_punctuations = text.translate(translator)\n","#     return text_without_punctuations"],"metadata":{"id":"hHJUV250oRHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# removed_punctuations_sentences = []\n","# for sentence in sentences:\n","#   removed_punctuations_sentences.append(remove_punctuation(sentence))\n","# removed_punctuations_sentences[:5]"],"metadata":{"id":"nC0_ic1BEHih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Tokenizing the text"],"metadata":{"id":"7gyDwlwJxj4K"}},{"cell_type":"code","source":["tokeninzed_text = []\n","for sentence in sentences:\n","  tokeninzed_text.append(word_tokenize(sentence))\n","tokeninzed_text[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6tFKpg0ubiy","executionInfo":{"status":"ok","timestamp":1706369449652,"user_tz":-420,"elapsed":3074,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"6a5a6913-7af3-496e-a0cb-f588166f1e4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A',\n"," 'Song',\n"," 'of',\n"," 'Ice',\n"," 'and',\n"," 'FireA',\n"," 'Game',\n"," 'of',\n"," 'ThronesPROLOGUEWe',\n"," 'should',\n"," 'start',\n"," 'back',\n"," ',',\n"," 'Gared',\n"," 'urged',\n"," 'as',\n"," 'the',\n"," 'woods',\n"," 'began',\n"," 'to',\n"," 'grow',\n"," 'dark',\n"," 'around',\n"," 'them',\n"," '.']"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["3. Numericalizing"],"metadata":{"id":"JQtvnU7303vK"}},{"cell_type":"code","source":["vocab = torchtext.vocab.build_vocab_from_iterator(tokeninzed_text, min_freq=3)\n","vocab.insert_token('<unk>', 0)\n","vocab.insert_token('<eos>', 1)\n","vocab.set_default_index(vocab['<unk>'])"],"metadata":{"id":"njqgsd7K8G9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the length\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suXK9GXN9w6x","executionInfo":{"status":"ok","timestamp":1706369449652,"user_tz":-420,"elapsed":15,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"af5c1927-9add-4e8c-8d59-953f05aa6187"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6354\n"]}]},{"cell_type":"code","source":["# Print some samples\n","print(vocab.get_itos()[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xZmdSdjd95Vg","executionInfo":{"status":"ok","timestamp":1706369449652,"user_tz":-420,"elapsed":14,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"73a5ea60-01a5-4d49-bd95-a4af014318a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '<eos>', ',', '.', 'the', 'and', 'to', 'of', 'a', 'his']\n"]}]},{"cell_type":"markdown","source":["**Prepare the batch loader**"],"metadata":{"id":"Ehoi4viH-7iv"}},{"cell_type":"code","source":["def get_data(dataset, vocab, batch_size):\n","    data = []\n","    for example in dataset:\n","        tokens = example.append('<eos>')\n","        tokens = [vocab[token] for token in example]\n","        data.extend(tokens)\n","    data = torch.LongTensor(data)\n","    num_batches = data.shape[0] // batch_size\n","    data = data[:num_batches * batch_size]\n","    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n","    return data #[batch size, seq len]\n","\n","    return data"],"metadata":{"id":"UhV7ZolbIlcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing the function\n","batch_size = 2\n","text_data = [word_tokenize('A Song of Ice and FireA Game of ThronesPROLOGUE.'),word_tokenize(' We should start back Gared urged as the woods began.')]\n","get_data(text_data, vocab, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KPmf47rIpSP","executionInfo":{"status":"ok","timestamp":1706369449652,"user_tz":-420,"elapsed":12,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"41c1d572-d1e7-4455-fbb6-070f03b871ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  84,    0,    7, 1539,    5,    0,    0,    7,    0,    3,    1],\n","        [ 163,  189, 1524,   61, 1201, 1337,   19,    4,  832,  271,    3]])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# Checking the shape\n","get_data(text_data, vocab, batch_size).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1N4VXnEJhVx","executionInfo":{"status":"ok","timestamp":1706369449652,"user_tz":-420,"elapsed":12,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"0a7d2231-9477-4658-d5c3-174cb0fb91ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 12])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["**Model**"],"metadata":{"id":"S3u0ROr90hPC"}},{"cell_type":"code","source":["class LSTMLanguageModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.hid_dim    = hid_dim\n","        self.emb_dim    = emb_dim\n","\n","        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n","        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n","        self.dropout    = nn.Dropout(dropout_rate)\n","        self.fc         = nn.Linear(hid_dim, vocab_size)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_range_emb = 0.1\n","        init_range_other = 1/math.sqrt(self.hid_dim)\n","        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n","        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n","        self.fc.bias.data.zero_()\n","        for i in range(self.num_layers):\n","            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n","                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n","            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,\n","                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n","\n","    def init_hidden(self, batch_size, device):\n","        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n","        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n","        return hidden, cell\n","\n","    def detach_hidden(self, hidden):\n","        hidden, cell = hidden\n","        hidden = hidden.detach() #not to be used for gradient computation\n","        cell   = cell.detach()\n","        return hidden, cell\n","\n","    def forward(self, src, hidden):\n","        #src: [batch_size, seq len]\n","        embedding = self.dropout(self.embedding(src)) #harry potter is\n","        #embedding: [batch-size, seq len, emb dim]\n","        output, hidden = self.lstm(embedding, hidden)\n","        #ouput: [batch size, seq len, hid dim]\n","        #hidden: [num_layers * direction, seq len, hid_dim]\n","        output = self.dropout(output)\n","        prediction =self.fc(output)\n","        #prediction: [batch_size, seq_len, vocab_size]\n","        return prediction, hidden"],"metadata":{"id":"gLKZSd1wy7h7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"oJ_lOR_5SPq0"}},{"cell_type":"markdown","source":["Initializing the parameters\n","\n","\n","1. The variables you provided are related to configuring a neural network model, and they are commonly used in the context of natural language processing (NLP) tasks, such as language modeling or machine translation. Let's go through each of them:\n","\n","2. vocab_size: This represents the size of the vocabulary, which is the total number of unique words in your dataset. In NLP tasks, each word is typically represented as a unique index, and the vocabulary size is the total number of unique indices.\n","\n","3. emb_dim: Short for embedding dimension, this parameter determines the size of the word embeddings. Word embeddings are vector representations of words in a continuous vector space. Each word in the vocabulary is mapped to a high-dimensional vector of emb_dim size. The embedding layer is used to convert discrete word indices into continuous vector representations.\n","\n","4. hid_dim: Hidden dimension is the size of the hidden state in recurrent neural networks (RNNs) or the size of the output dimension in feedforward neural networks. In the context of an RNN, it represents the number of hidden units in each recurrent layer. Larger hidden dimensions can capture more complex patterns but may also require more computational resources.\n","\n","5. num_layers: This parameter indicates the number of layers in the neural network. In the context of recurrent neural networks (RNNs), it represents the number of recurrent layers. For feedforward neural networks, it represents the number of hidden layers.\n","\n","6. dropout_rate: Dropout is a regularization technique used to prevent overfitting in neural networks. During training, a random fraction of the neurons is dropped out (i.e., their outputs are set to zero) at each update, which helps prevent co-adaptation of hidden units. The dropout_rate is the probability of dropping out a neuron during training.\n","\n","7. lr: Learning rate is a hyperparameter that determines the step size at each iteration during the optimization process. It controls how much the model's weights should be updated during training. A smaller learning rate may lead to more stable convergence, but training might be slower, while a larger learning rate may speed up training but risk overshooting the optimal weights."],"metadata":{"id":"OD4A9DC-Vl-c"}},{"cell_type":"code","source":["vocab_size = len(vocab)\n","emb_dim = 1024\n","hid_dim = 1024\n","num_layers = 2\n","dropout_rate = 0.65\n","lr = 1e-3"],"metadata":{"id":"qaw8mZ-pSD1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n","optimizer  = optim.Adam(model.parameters(), lr=lr)\n","criterion  = nn.CrossEntropyLoss()\n","num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'The model has {num_params:,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dR8qEYNCSSu8","executionInfo":{"status":"ok","timestamp":1706315697951,"user_tz":-420,"elapsed":2771,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"a2de8eea-3bcb-4ecf-a3c4-b7d2b33b26d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 29,812,946 trainable parameters\n"]}]},{"cell_type":"code","source":["def get_batch(data, seq_len, idx):\n","    #data #[batch size, bunch of tokens]\n","    src    = data[:, idx:idx+seq_len]\n","    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1\n","    return src, target"],"metadata":{"id":"1kBq5gJWSUOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n","\n","    epoch_loss = 0\n","    model.train()\n","    # drop all batches that are not a multiple of seq_len\n","    # data #[batch size, seq len]\n","    num_batches = data.shape[-1]\n","    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n","    num_batches = data.shape[-1]\n","\n","    #reset the hidden every epoch\n","    hidden = model.init_hidden(batch_size, device)\n","\n","    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n","        optimizer.zero_grad()\n","\n","        #hidden does not need to be in the computational graph for efficiency\n","        hidden = model.detach_hidden(hidden)\n","\n","        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n","        src, target = src.to(device), target.to(device)\n","        batch_size = src.shape[0]\n","        prediction, hidden = model(src, hidden)\n","\n","        #need to reshape because criterion expects pred to be 2d and target to be 1d\n","        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]\n","        target = target.reshape(-1)\n","        loss = criterion(prediction, target)\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += loss.item() * seq_len\n","    return epoch_loss / num_batches"],"metadata":{"id":"L2DNlw3PSfJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data, criterion, batch_size, seq_len, device):\n","\n","    epoch_loss = 0\n","    model.eval()\n","    num_batches = data.shape[-1]\n","    data = data[:, :num_batches - (num_batches -1) % seq_len]\n","    num_batches = data.shape[-1]\n","\n","    hidden = model.init_hidden(batch_size, device)\n","\n","    with torch.no_grad():\n","        for idx in range(0, num_batches - 1, seq_len):\n","            hidden = model.detach_hidden(hidden)\n","            src, target = get_batch(data, seq_len, idx)\n","            src, target = src.to(device), target.to(device)\n","            batch_size= src.shape[0]\n","\n","            prediction, hidden = model(src, hidden)\n","            prediction = prediction.reshape(batch_size * seq_len, -1)\n","            target = target.reshape(-1)\n","\n","            loss = criterion(prediction, target)\n","            epoch_loss += loss.item() * seq_len\n","    return epoch_loss / num_batches"],"metadata":{"id":"SWoXoCY1SiYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["size_of_training_data = int(0.8 * len(tokeninzed_text))\n","train_data = tokeninzed_text[:size_of_training_data]\n","valid_data = tokeninzed_text[size_of_training_data:]"],"metadata":{"id":"0XBlktWUSqZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","train_data = get_data(train_data, vocab, batch_size)\n","valid_data = get_data(valid_data, vocab, batch_size)\n","#test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"],"metadata":{"id":"imYcbemeTvKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs = 20\n","seq_len  = 50 #<----decoding length\n","clip    = 0.25\n","\n","lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(n_epochs):\n","    train_loss = train(model, train_data, optimizer, criterion,\n","                batch_size, seq_len, clip, device)\n","    valid_loss = evaluate(model, valid_data, criterion, batch_size,\n","                seq_len, device)\n","\n","    lr_scheduler.step(valid_loss)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/NLP_A2/best-val-lstm_lm.pt')\n","\n","    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n","    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7koEUmGSj6u","outputId":"18bc72bc-068b-459f-90c8-55fafddaec71","executionInfo":{"status":"ok","timestamp":1706334785523,"user_tz":-420,"elapsed":19063475,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 580.741\n","\tValid Perplexity: 446.872\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 342.097\n","\tValid Perplexity: 260.573\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 210.871\n","\tValid Perplexity: 202.781\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 163.193\n","\tValid Perplexity: 166.514\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 137.254\n","\tValid Perplexity: 149.642\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 121.336\n","\tValid Perplexity: 137.163\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 109.599\n","\tValid Perplexity: 128.855\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 100.258\n","\tValid Perplexity: 121.452\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 92.585\n","\tValid Perplexity: 116.136\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 86.683\n","\tValid Perplexity: 112.125\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 81.605\n","\tValid Perplexity: 108.380\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 76.992\n","\tValid Perplexity: 106.348\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 73.050\n","\tValid Perplexity: 104.200\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 69.501\n","\tValid Perplexity: 102.455\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 66.640\n","\tValid Perplexity: 101.330\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 63.871\n","\tValid Perplexity: 100.422\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 61.577\n","\tValid Perplexity: 100.193\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 59.559\n","\tValid Perplexity: 100.015\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 57.225\n","\tValid Perplexity: 99.358\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tTrain Perplexity: 55.270\n","\tValid Perplexity: 99.041\n"]}]},{"cell_type":"code","source":["def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","    model.eval()\n","    tokens = tokenizer(prompt)\n","    indices = [vocab[t] for t in tokens]\n","    batch_size = 1\n","    hidden = model.init_hidden(batch_size, device)\n","    with torch.no_grad():\n","        for i in range(max_seq_len):\n","            src = torch.LongTensor([indices]).to(device)\n","            prediction, hidden = model(src, hidden)\n","\n","            #prediction: [batch size, seq len, vocab size]\n","            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n","\n","            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n","            prediction = torch.multinomial(probs, num_samples=1).item()\n","\n","            while prediction == vocab['<unk>']: #if it is unk, we sample again\n","                prediction = torch.multinomial(probs, num_samples=1).item()\n","\n","            if prediction == vocab['<eos>']:    #if it is eos, we stop\n","                break\n","\n","            indices.append(prediction) #autoregressive, thus output becomes input\n","\n","    itos = vocab.get_itos()\n","    tokens = [itos[i] for i in indices]\n","    return tokens"],"metadata":{"id":"SKuYTsnobFco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = 'Royce nodded'\n","max_seq_len = 30\n","seed = 0\n","\n","#smaller the temperature, more diverse tokens but comes\n","#with a tradeoff of less-make-sense sentence\n","temperatures = [1, 1, 0.5, 1.0, 1.0]\n","for temperature in temperatures:\n","    generation = generate(prompt, max_seq_len, temperature, model, word_tokenize,\n","                          vocab, device, seed)\n","    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_UGl4dDbF5h","executionInfo":{"status":"ok","timestamp":1706335711750,"user_tz":-420,"elapsed":2491,"user":{"displayName":"Pirunrat Kaewphanoaw","userId":"16190790272292098917"}},"outputId":"1148d571-4ae9-4be6-ec04-6397b19b50dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Royce nodded , a heavy doublet red and swaying .\n","\n","1\n","Royce nodded , a heavy doublet red and swaying .\n","\n","0.5\n","Royce nodded , a heavy voice of the Eyrie .\n","\n","1.0\n","Royce nodded , a heavy doublet red and swaying .\n","\n","1.0\n","Royce nodded , a heavy doublet red and swaying .\n","\n"]}]},{"cell_type":"markdown","source":["**Save the files**"],"metadata":{"id":"sILVYpGmkbqn"}},{"cell_type":"markdown","source":["Vocab"],"metadata":{"id":"QO-VFNSYk6dI"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/NLP_A2/vocabs.pkl', 'wb') as f:\n","    pickle.dump(vocab, f)"],"metadata":{"id":"WDiJTklsgxKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rDw3dqUUk86H"},"execution_count":null,"outputs":[]}]}